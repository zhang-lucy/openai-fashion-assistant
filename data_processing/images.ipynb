{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914ec9b5-5feb-4946-8dd1-9ab416d2179f",
   "metadata": {},
   "source": [
    "# Images\n",
    "Experimented with BLIP to create captions for images. Though our text data is pretty comprehensive and we don't _need_ image data, the caption from the images will likely provide additional context sometimes not explicitly written in metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ddcc166-83d5-4588-bfdf-e09d41d92fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load once at the top-level to avoid re-downloading models\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def generate_caption(image_url: str) -> str:\n",
    "    \"\"\"Given an image URL, returns a caption generated by BLIP.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        output = model.generate(**inputs)\n",
    "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return f\"Error processing image: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8310eb-1843-44ee-a0b7-165a0de85922",
   "metadata": {},
   "source": [
    "![bra](https://m.media-amazon.com/images/I/51mCnuqW6HL._AC_.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f926da-5468-431c-bd4a-828f76b3c9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a woman wearing a white bra with floral print'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_caption(\"https://m.media-amazon.com/images/I/51mCnuqW6HL._AC_.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbdf8e-a9d6-4551-a4e9-3a570d7d3e49",
   "metadata": {},
   "source": [
    "![socks](https://m.media-amazon.com/images/I/41wv1FeF3fL._AC_.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10e7e65-494e-4f3a-b6ab-ec6839de63df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a pair of white socks with red and black accents'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_caption(\"https://m.media-amazon.com/images/I/41wv1FeF3fL._AC_.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdabe6bf-97bd-4e5b-a4a2-6d61173a69ee",
   "metadata": {},
   "source": [
    "![band](https://m.media-amazon.com/images/I/51cQ43xwDoL._AC_.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f92f839-ae37-4c27-8884-00b6b8b153a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the watch strap is made from woven fabric'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_caption(\"https://m.media-amazon.com/images/I/51cQ43xwDoL._AC_.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90214dac-36bb-4aea-b00b-3b496a3d5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b8c461-45a7-4340-827a-60a56974b891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_url</th>\n",
       "      <th>primary_keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>['ekouaer-womens-long-nightgown-short-sleeve-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://m.media-amazon.com/images/I/01+4kROOdv...</td>\n",
       "      <td>['aviatrix-mens-boys-us-air-g-force-pilot-blac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://m.media-amazon.com/images/I/01+5esf5ol...</td>\n",
       "      <td>['women-zip-up-hoodies-casual-sweatshirts-drop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://m.media-amazon.com/images/I/01+7HdDcbv...</td>\n",
       "      <td>['haola-womens-casual-print-loose-crewneck-sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://m.media-amazon.com/images/I/01+K2hCumd...</td>\n",
       "      <td>['pin-high-ricardo-dry-fit-high-performance-go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_url  \\\n",
       "0                                                NaN   \n",
       "1  https://m.media-amazon.com/images/I/01+4kROOdv...   \n",
       "2  https://m.media-amazon.com/images/I/01+5esf5ol...   \n",
       "3  https://m.media-amazon.com/images/I/01+7HdDcbv...   \n",
       "4  https://m.media-amazon.com/images/I/01+K2hCumd...   \n",
       "\n",
       "                                        primary_keys  \n",
       "0  ['ekouaer-womens-long-nightgown-short-sleeve-n...  \n",
       "1  ['aviatrix-mens-boys-us-air-g-force-pilot-blac...  \n",
       "2  ['women-zip-up-hoodies-casual-sweatshirts-drop...  \n",
       "3  ['haola-womens-casual-print-loose-crewneck-sho...  \n",
       "4  ['pin-high-ricardo-dry-fit-high-performance-go...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"image_urls.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de230118-2aef-4e84-9dd1-ccd616670232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def process_images(df: pd.DataFrame, start_index: int = 0, output_dir: str = \"image_captions\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i, row in df.iloc[start_index:].iterrows():\n",
    "        image_url = row[\"image_url\"]\n",
    "        primary_keys = row[\"primary_keys\"]\n",
    "\n",
    "        caption = generate_caption(image_url)\n",
    "\n",
    "        result = {\n",
    "            \"image_url\": image_url,\n",
    "            \"primary_keys\": primary_keys,\n",
    "            \"caption\": caption\n",
    "        }\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"caption_{i}.json\")\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        print(f\"[{i}] Wrote caption to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6c99191-62bf-49f0-b285-3d633806bc02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] Wrote caption to image_captions/caption_10.json\n",
      "[11] Wrote caption to image_captions/caption_11.json\n",
      "[12] Wrote caption to image_captions/caption_12.json\n",
      "[13] Wrote caption to image_captions/caption_13.json\n",
      "[14] Wrote caption to image_captions/caption_14.json\n",
      "[15] Wrote caption to image_captions/caption_15.json\n",
      "[16] Wrote caption to image_captions/caption_16.json\n",
      "[17] Wrote caption to image_captions/caption_17.json\n",
      "[18] Wrote caption to image_captions/caption_18.json\n",
      "[19] Wrote caption to image_captions/caption_19.json\n",
      "[20] Wrote caption to image_captions/caption_20.json\n",
      "[21] Wrote caption to image_captions/caption_21.json\n",
      "[22] Wrote caption to image_captions/caption_22.json\n",
      "[23] Wrote caption to image_captions/caption_23.json\n",
      "[24] Wrote caption to image_captions/caption_24.json\n",
      "[25] Wrote caption to image_captions/caption_25.json\n",
      "[26] Wrote caption to image_captions/caption_26.json\n",
      "[27] Wrote caption to image_captions/caption_27.json\n",
      "[28] Wrote caption to image_captions/caption_28.json\n",
      "[29] Wrote caption to image_captions/caption_29.json\n",
      "[30] Wrote caption to image_captions/caption_30.json\n",
      "[31] Wrote caption to image_captions/caption_31.json\n",
      "[32] Wrote caption to image_captions/caption_32.json\n",
      "[33] Wrote caption to image_captions/caption_33.json\n",
      "[34] Wrote caption to image_captions/caption_34.json\n",
      "[35] Wrote caption to image_captions/caption_35.json\n",
      "[36] Wrote caption to image_captions/caption_36.json\n",
      "[37] Wrote caption to image_captions/caption_37.json\n",
      "[38] Wrote caption to image_captions/caption_38.json\n",
      "[39] Wrote caption to image_captions/caption_39.json\n",
      "[40] Wrote caption to image_captions/caption_40.json\n",
      "[41] Wrote caption to image_captions/caption_41.json\n",
      "[42] Wrote caption to image_captions/caption_42.json\n",
      "[43] Wrote caption to image_captions/caption_43.json\n",
      "[44] Wrote caption to image_captions/caption_44.json\n",
      "[45] Wrote caption to image_captions/caption_45.json\n",
      "[46] Wrote caption to image_captions/caption_46.json\n",
      "[47] Wrote caption to image_captions/caption_47.json\n",
      "[48] Wrote caption to image_captions/caption_48.json\n",
      "[49] Wrote caption to image_captions/caption_49.json\n",
      "[50] Wrote caption to image_captions/caption_50.json\n",
      "[51] Wrote caption to image_captions/caption_51.json\n",
      "[52] Wrote caption to image_captions/caption_52.json\n",
      "[53] Wrote caption to image_captions/caption_53.json\n",
      "[54] Wrote caption to image_captions/caption_54.json\n",
      "[55] Wrote caption to image_captions/caption_55.json\n",
      "[56] Wrote caption to image_captions/caption_56.json\n",
      "[57] Wrote caption to image_captions/caption_57.json\n",
      "[58] Wrote caption to image_captions/caption_58.json\n",
      "[59] Wrote caption to image_captions/caption_59.json\n",
      "[60] Wrote caption to image_captions/caption_60.json\n",
      "[61] Wrote caption to image_captions/caption_61.json\n",
      "[62] Wrote caption to image_captions/caption_62.json\n",
      "[63] Wrote caption to image_captions/caption_63.json\n",
      "[64] Wrote caption to image_captions/caption_64.json\n",
      "[65] Wrote caption to image_captions/caption_65.json\n",
      "[66] Wrote caption to image_captions/caption_66.json\n",
      "[67] Wrote caption to image_captions/caption_67.json\n",
      "[68] Wrote caption to image_captions/caption_68.json\n",
      "[69] Wrote caption to image_captions/caption_69.json\n",
      "[70] Wrote caption to image_captions/caption_70.json\n",
      "[71] Wrote caption to image_captions/caption_71.json\n",
      "[72] Wrote caption to image_captions/caption_72.json\n",
      "[73] Wrote caption to image_captions/caption_73.json\n",
      "[74] Wrote caption to image_captions/caption_74.json\n",
      "[75] Wrote caption to image_captions/caption_75.json\n",
      "[76] Wrote caption to image_captions/caption_76.json\n",
      "[77] Wrote caption to image_captions/caption_77.json\n",
      "[78] Wrote caption to image_captions/caption_78.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# df = pd.read_json(\"grouped_images.json\")  # Make sure it has image_url + primary_keys columns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mprocess_images\u001b[39m\u001b[34m(df, start_index, output_dir)\u001b[39m\n\u001b[32m      7\u001b[39m image_url = row[\u001b[33m\"\u001b[39m\u001b[33mimage_url\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m primary_keys = row[\u001b[33m\"\u001b[39m\u001b[33mprimary_keys\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m caption = \u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m result = {\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage_url\u001b[39m\u001b[33m\"\u001b[39m: image_url,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprimary_keys\u001b[39m\u001b[33m\"\u001b[39m: primary_keys,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcaption\u001b[39m\u001b[33m\"\u001b[39m: caption\n\u001b[32m     16\u001b[39m }\n\u001b[32m     18\u001b[39m output_path = os.path.join(output_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcaption_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mgenerate_caption\u001b[39m\u001b[34m(image_url)\u001b[39m\n\u001b[32m     12\u001b[39m image = Image.open(requests.get(image_url, stream=\u001b[38;5;28;01mTrue\u001b[39;00m).raw).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m inputs = processor(images=image, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m caption = processor.decode(output[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m caption\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/transformers/models/blip/modeling_blip.py:1194\u001b[39m, in \u001b[36mBlipForConditionalGeneration.generate\u001b[39m\u001b[34m(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)\u001b[39m\n\u001b[32m   1161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1162\u001b[39m \u001b[33;03mOverrides *generate* function to be able to use the model as a conditional generator\u001b[39;00m\n\u001b[32m   1163\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m   1191\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1193\u001b[39m batch_size = pixel_values.shape[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1194\u001b[39m vision_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m image_embeds = vision_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1201\u001b[39m image_attention_mask = torch.ones(image_embeds.size()[:-\u001b[32m1\u001b[39m], dtype=torch.long, device=image_embeds.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/transformers/models/blip/modeling_blip.py:735\u001b[39m, in \u001b[36mBlipVisionModel.forward\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[39m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify pixel_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    733\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m last_hidden_state = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    743\u001b[39m last_hidden_state = \u001b[38;5;28mself\u001b[39m.post_layernorm(last_hidden_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/transformers/models/blip/modeling_blip.py:674\u001b[39m, in \u001b[36mBlipEncoder.forward\u001b[39m\u001b[34m(self, inputs_embeds, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    667\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    668\u001b[39m         encoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    669\u001b[39m         hidden_states,\n\u001b[32m    670\u001b[39m         attention_mask,\n\u001b[32m    671\u001b[39m         output_attentions,\n\u001b[32m    672\u001b[39m     )\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/transformers/models/blip/modeling_blip.py:453\u001b[39m, in \u001b[36mBlipEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions)\u001b[39m\n\u001b[32m    451\u001b[39m residual = hidden_states\n\u001b[32m    452\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    455\u001b[39m hidden_states = hidden_states + residual\n\u001b[32m    457\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/transformers/models/blip/modeling_blip.py:411\u001b[39m, in \u001b[36mBlipMLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.activation_fn(hidden_states)\n\u001b[32m    413\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.fc2(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/openai-takehome/data_processing/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# df = pd.read_json(\"grouped_images.json\")  # Make sure it has image_url + primary_keys columns\n",
    "process_images(df, start_index=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d09f33d-547d-44af-83bc-7f6072c372eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651148"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33852f88-1ce5-433d-b585-961d5c57d416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI Interview Env",
   "language": "python",
   "name": "fashion-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
